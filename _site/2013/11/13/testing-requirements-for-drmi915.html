<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Testing Requirements for drm/i915 Features and Patches</title>
  <meta name="description" content="I want to make automated test coverage an integral part of our feature and bugfix development process. For features this means that starting with the design ...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/2013/11/13/testing-requirements-for-drmi915.html">
  <link rel="alternate" type="application/rss+xml" title="stuff by danvet" href="http://yourdomain.com/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">stuff by danvet</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Testing Requirements for drm/i915 Features and Patches</h1>
    <p class="post-meta"><time datetime="2013-11-13T00:49:00+01:00" itemprop="datePublished">Nov 13, 2013</time> â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">danvet</span></span></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    I want to make automated test coverage an integral part of our feature and bugfix development process. For features this means that starting with the design phase testability needs to be considered an integral part of any feature. This needs to go through the entire development process, from planning, development, patch submission and final validation. For bugfixes that means the fix is only complete once the automated testcase for it is also done, if we need a new one<br /><br />This specifically excludes testing with humans somewhere in the loop. We are extremely limited in our validation resources, every time we put something new onto the "manual testing" plate something else <i>will</i> fall off. <br /><br />  I've let this float for quite a while both internally in Intel and on the public mailing lists. Thanks to everyone who provided valuable input. Essentially this just codifies the already existing expectations from me as the maintainer, but those requirements haven't really been clear and a lot of emotional discussions ensued. With this we should now have solid guidelines and can go back to coding instead of blowing through so much time and energy on waging flamewars. <br /><br /><a name='more'></a><h2>Why?</h2>There are a bunch of reasons why having good tests is great: <ul><li><b>More predictability.</b> Right now test coverage often only comes up as a topic   when I drop my maintainer review onto a patch series. Which is too late, since   it'll delay the otherwise working patches and so massively frustrates people.   I hope by making test requirements clear and up-front we can make the   upstreaming process more predictable. Also, if we have good tests from the get-go   there should be much less need for me to drop patches from my trees   after having them merged.</li><li><b>Less bikeshedding.</b> In my opinion test cases are an excellent means to settle   bikesheds - we've had in the past seen cases of endless back&amp;forths   where writing a simple testcase would have shown that <i>all</i> proposed   color flavours are actually broken.<br />The even more important thing is that fully automated tests allow us to   legitimately postpone cleanups. If the only testing we have is manual testing   then we have only one shot at a feature tested, namely when the developer   tests it. So it better be perfect. But with automated tests we can postpone   cleanups with too high risks of regressions until a really clear need is   established. And since that need often never materializes we'll save work.</li><li><b>Better review.</b> For me it's often helps a lot to review tests than the actual   code in-depth. This is especially true for reviewing userspace interface   additions.</li><li><b>Actionable regression reports.</b> Only if we have a fully automated testcase do   we have a good chance that QA reports a regression within just a few days.   Everything else can easily take weeks (for platforms and features which are   explicitly tested) to months (for stuff only users from the community notice).   And especially now that much more shipping products depend upon a working   i915.ko driver we just can't do this any more.</li><li><b>Better tests.</b> A lot of our code is really hard to test in an automated   fashion, and pushing the frontier of what is testable often requires a lot of   work. I hope that by making tests an integral part of any feature work and so   forcing more people to work on them and think about testing we'll   advance the state of the art at a brisker pace. </ul><br /><h2>Risks and Buts</h2> But like with every change, not everything is all glorious and fun: <ul><li><b>Bikeshedding on tests.</b> This plan is obviously not too useful if we just   replace massive bikeshedding on patches with massive bikeshedding on   testcases. But right now we do almost no review on i-g-t patches so the risk   is small. Long-term the review requirements for testcases will certainly   increase, but as with everything else we simply need to strive for a good   balance to strike for just the right amount of review. <br />Also if we really start discussing tests <i>before</i> having written massive patch   series we'll do the bikeshedding while there's no real rebase pain. So even if   the bikeshedding just shifts we'll benefit I think, especially for   really big features.</li><li><b>Technical debt in test coverage.</b> We have a lot of old code which still   completely lacks testcases. Which means that even small feature work might be   on the hook for a big pile of debt restructuring. I think this is inevitable   occasionally. But I think that doing an assessment of the current state of   test coverage of the existing code <i>before</i> starting a feature instead   of when the patches are ready for merging should help a lot, before   everyone is invested into patches already and mounting rebase pain looms   large.<br />Again we need to strive for a good balance between "too many tests to write   up-front for old code" and "missing tests that only the final review and validation   uncovers creating process bubbles".</li><li><b>Upstreaming of product stuff.</b> Product guys are notoriously busy and writing   tests is actual work. On the other hand the upstream codebase feeds back into <i>all</i> product   trees (and the upstream kernel), so requirements are simply a bit higher. And   I also don't think that we can push the testing of some features fully to   product teams, since they'll be pissed really quickly if every update they get   from us breaks their stuff. So if these additional test requirements (compared   to the past) means that some product patches won't get merged, then I think   that's the right choice.</li><li><b>But ...</b> all the other kernel drivers don't do this. We're also one of the   biggest driver's in the kernel, with a code churn rate roughly 5x worse than   anything else and a pretty big (and growing) team. Also, we're often the   critical path in enabling new platforms in the fast-paced mobile space.   Different standards apply.</li></ul><br/><h2>Test Coverage Expectations</h2> Since the point here is to make the actual test requirements known up-front we need to settle on clear expectations. <ul><li>Tests must fully cover userspace interfaces. By this I mean exercising all the   possible options, especially the usual tricky corner cases (e.g. off-by-one   array sizes, overflows). It also needs to include tests for all the   userspace input validation (i.e. correctly rejecting invalid input,   including checks for the error codes). For userspace interface additions   technical debt really must be addressed. This means that when adding a   new flag and we currently don't have any tests for those flags, then   I'll ask for a testcase which fully exercises all the flag values we   currently supported on top of the new interface addition.</li><li>Tests need to provide a reasonable baseline coverage of the internal driver   state. The idea here isn't to aim for full coverage, that's an impossible and   pointless endeavor. The goal is to have a good starting point of tests so that   when a tricky corner case pops up in review or validation that it's not a   terribly big effort to add a specific testcase for it. This is very much a balance thing to get right and we need a bit of experience to get a good handle here.</li><li>Issues discovered in review and final validation need automated test coverage. This includes any bugs found after a feature has already landed and is even more important for regressions.   The reasoning is that anything which slipped the developer's attention is   tricky enough to warrant an explicit testcase, since in a later refactoring   there's a good chance that it'll be missed again. This has a bit a risk   to delay patches, but if the basic test coverage is good enough as per   the previous point it really shouln't be an issue.</li><li>Finally we need to push the testable frontier with new ideas like pipe CRCs,   modeset state cross checking or arbitrary monitor configuration injection   (with fixed EDIDs and connector state forcing). The point here is to foster   new crazy ideas, and the expectation is very much <i>not</i> that developers then   need to write testcases for all the old bugfixes that suddenly became   testable. That workload needs to be spread out over a bunch of features   touching the relevant area. This only really applies to features and   code paths which are currently in the "not testable" bucket anyway.</li></ul>This should specify the "what" decently enough, but we also need to look at how tests should work.<br /><br />Specific testcases in i-g-t are obviously the preferred form, but for some features that's just not possible. In such cases in-kernel self-checks like the modeset state checker of fifo underrun reporting are really good approaches. Two caveats apply: <ul><li>The test infrastructure really should be orthogonal to the code being tested.   In-line asserts that check for preconditions are really nice and useful, but   since they're closely tied to the code itself they have a good chance to be broken   in the same ways.</li><li>The debug feature needs to be enabled by default, and it needs to be loud.   Otherwise no one will notice that something is amiss. So currently the fifo   underrun reporting doesn't really count since it only causes debug level   output when something goes wrong. Of course it's still a really good tool for   developers, just not yet for catching regressions.</li></ul>Finally the short lists of excuses that don't count as proper test coverage for a feature: <ul><li>Manual testing. We are ridiculously limited on our QA manpower. Every time we   drop something onto the "manual testing" plate something else <i>will</i> drop off.   Which means in the end that we don't really have any test coverage. So   if patches don't come with automated tests, in-kernel cross-checking or   some other form of validation attached they need to have really good   reasons for doing so.</li><li>Testing by product teams. The entire point of Intel OTC's "upstream first"   strategy is to have a common codebase for everyone. If we break product trees   every time we feed an update into them because we can't properly regression   test a given feature then the value of upstreaming features is greatly   diminished. In my opinion this could potentially doom collaborations with   product teams. We just can't have that.<br/>This means that when products teams submit patches upstream they also need   to submit the relevant testcases as patches to i-g-t.  </li></ul><br/><h2>Process Adjustements</h2>The important piece is really to not start with thinking about tests only when everything else is done. <ul><li>For big features we should have an upfront discussion about the test coverage and what all should be done (like any coverage gaps for existing code and features to fill, a new crazy test infrastructure idea to implement as a proof of concept or what kinds of tests would provide a reasonable base coverage). For really big features writing a quick test plan and everyone signing off on it could be useful. Especially to be able to learn and improve once everything has landed and the usefulness of the tests is much clearer.</li><li>Tests should be implemented together with the feature or bugfix and should be ready about the same time. Having both pieces at hand should help development, testing and review.</li><li>If we decide that new test infrastructure is required or that there's a large gap in the coverage of existing code then that should be done before the main feature is developed. Otherwise we'll suffer again the pains of rebase hell for no gain.</li><li> Finally developers are <i>not</i> expected to run the full testsuite before submitting patches. The test suite currently simply takes too long to run and we don't have any good centralized infrastructure to speed things up by running tests on multiple machines in parallel. And then proper testing requires a wide array of different platforms anyway, so full regression testing is still squarely a job for our QA. Of course we need to improve our infrastructure and also make it easier to run a useful subset of tests while developing patches.</li></ul>With that, happy testcase writing to everyone! 
  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">stuff by danvet</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>stuff by danvet</li>
          <li><a href="mailto:your-email@domain.com">your-email@domain.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/danvet"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">danvet</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/danvet"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">danvet</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
