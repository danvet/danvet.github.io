<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>i915/GEM Crashcourse, Part 4</title>
  <meta name="description" content="In the previous installment we've taken a closer look at some details of the gpu memory management. One of the last big topics now still left are all the var...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/2013/01/07/i915gem-crashcourse-part-4.html">
  <link rel="alternate" type="application/rss+xml" title="stuff by danvet" href="http://yourdomain.com/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">stuff by danvet</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">i915/GEM Crashcourse, Part 4</h1>
    <p class="post-meta"><time datetime="2013-01-07T16:56:00+01:00" itemprop="datePublished">Jan 7, 2013</time> â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">danvet</span></span></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>In the previous installment we've taken a closer look at some <a href="http://blog.ffwll.ch/2012/11/i915gem-crashcourse-part-3.html">details of the gpu memory management</a>. One of the last big topics now still left are all the various caches, both on the gpu (both render and display block) and the cpu, and what is required to keep the data coherent between them. Now one of the reasons gpus are so fast at processing raw amounts of data is that caches are managed through explicit instructions (cutting down massively on complexity and delays) and there are also a lot of special-purpose caches optimized for different use-cases. Since coherency management isn't automatic, we will also consider the different ways to move data between different coherency domains and what the respective up- and downsides are. See the <a href="http://blog.ffwll.ch/2013/01/i915gem-crashcourse-overview.html">i915/GEM crashcourse overview</a> for links to the other parts of this series.  </p><a name='more'></a> <h2>Interactions with CPU Caches</h2> <p>The first thing to look at is how gpu-side caches work together with the cpu caches. Traditionally the intel gfx has been sitting right on top of the memory controller. For maximum efficiency it therefore did not take part in the coherency protocol with the cpu caches, but had direct read/write access to the underlying memory. To ensure that data written by the cpu could be read by the gpu coherently, we need to flush cpu caches and further the chipset write queue on the separate memory controller. Only once the data is guaranteed to have reached physical memory can it be read by the gpu. </p> <p>For readbacks, e.g. after the gpu has rendered a frame, we again need to invalidate all the cpu caches - the gpu writes don't snoop cpu caches and so the cpu cache might still contain stale data. On intel hw this is again done by the clflush instruction, which (only on Intel cpus though) guarantees that clean cachelines will simply be dropped and not written back to memory. This is important, since otherwise data in memory written by the gpu could be overwritten by stale cache contents from the cpu. </p> <p>Now on newer platforms (Sandybridge and later) where the gpu is integrated with the cpu on the same die, the render portion of the gpu is sitting on top of the last level caches, together with all the cpu cores. Current architecture is that the cpu cores (including L2 caches), gpu (again including gpu specific caches), memory controller and the big L3 cache are all connected with a coherent ring interconnect. Since then the gpu can profit from the big L3 cache on the die, giving a decent speedup for gpu workloads. Hence it is now beneficial for gpu reads/writes to be coherent with all the cpu caches - explicitly flushing cachelines is cumbersome and the last-level cache can easily hide the snooping latencies. </p> <p>The odd thing out on this architecture is the display unit. To preserve the most power in idle situation we want to be able to turn off power for cpu cores, the gpu render block and the entire L3 and interconnect (called uncore). Which means the always-on display block can only read from memory directly and can't be coherent with caches. To allow this we can specify the caching mode in the gtt pagetables and so mark render targets which will be used as scanout sources uncacheable. But that also implies that they're no longer coherent with cpu cache contents, and so we need to again engage in manual cacheline flushing like on the platforms without a shared last level cache. </p> <h2>Transferring Data Between CPU and GPU Coherency Domains</h2> <p>The oldest way to move data between the gpu and cpu in the i915 GEM kernel driver was with the <code>set_domain</code> ioctl. If required, it flushed the cpu caches for all the cachelines for the entire memory address range of the object. Which is really expensive, so we need more efficient ways to move date from/to the gpu, especially on platforms without a coherent last level cache. Note that all objects start out in the cpu domain, since freshly-allocated pages aren't necessarily flushed. Hence it is important that userspace keeps around a cache of already flushed objects, to amortize the expensive initial cacheline flushing. Also note that the <code>set_domain</code> ioctl is also used to synchronize cpu access with outstanding gpu rendering, and so still used in that role for coherent objects. </p>  <p>For this the kernel and older hw support a snooping mode, where the gpu asks the cpu to clear any dirty cachelines before reading/write. The upside is that we don't blow through cpu time flushing cachelines, and the gpu seems to be more efficient at moving data out of and into cpu caches anyway. The downside is, at least on older generations, that we can't use such snooped areas for rendering, but only to offload data transfers from/to the cpu to gpu. But since there are now downsides for the cpu in accessing snooped gem buffer objects they are very interesting for mixed sw/hw rendering. Another upside is that the uploads can be streamed and are asynchronously done, so avoiding stalls. Downloads can obviously also be done asynchronously using the gpu, but additional unrelated queued rendering could introduce stalls since the cpu often needs the data right away. </p>  <p>Hence the tradeoffs for picking the gpu for copieding data from/to snoopable buffers compared to using the cpu to transfer data is tricky: For uploads it's usually only worth it to use the gpu to avoid stalls, since our cpu uploads paths are fast. For downloads on platforms without a shared last-level cached gpu copies using snoopable buffers beats the cpu readbacks hands-down. </p> <p>The i915 GEM driver exposes a set of ioctls to allow userspace to read/write linear buffers which try to pick the most efficient way to transfer the data from/to the gpu coherency domain, called <code>pwrite</code> and <code>pread</code>. We'll later on look at some of the trade-offs and tricks employed, but for now only discuss how reads/writes which only partially cover a cacheline need to be handled if we write through the cpu mappings. Reads are simple, every cacheline we touch needs to be flush before reading data written by the gpu. For writes we only need to flush before writing if the cacheline is partially covered - stale data from cpu caches in that cacheline could otherwise be written to memory. For writes covering entire cachelines we can avoid this. Hence userspace tries to avoid such partial writes. Obviously we also need to flush the cacheline after writing to it, to move the data out to the gpu. </p> <p>All this flushing doesn't apply when the buffer is coherent, and generally on platforms with a last-level cache writing through the cpu caches is the most efficient way to copy data with the cpu (the gpu can obviously also copy things around, since it doesn't pay a penalty for coherent access on such platforms). Going through the GTT to e.g. avoid manually tiling and swizzling a texture is much slower. The reason for that is that GTT writes land directly in main memory and don't go into the last-level cache - hence we're limited by main memory bandwidth and can't benefit from the fast caches. It does though issue a snoop to ensure that everything is still coherent. </p> <p>On platforms without a shared last-level cache it's a completely different story though - we have to manually flush cpu caches, which is a rather expensive operation. GTT writes on the other hand bypass all caches and land directly in main memory. Which means that on these platforms GTT writes are generally 2-3x faster than writes through the cpu caches - a notch slower if the GTT area written to is tiled. GTT reads are in all cases extremely slow since the GTT I/O range is only write-combined, but uncached. </p> <p>Now one of the clever tricks that the pwrite ioctl implements is that on platforms where GTT writes are faster it tries to first move the buffer object into the mappable part of the GTT (if the object isn't there yet and this can be done without blocking for the gpu). This way the upload benefits from the faster GTT writes, which easily offsets any costs in unbinding a few other buffer objects and rewriting a bunch of PTEs. Especially since on recent kernels and most platforms GTT PTEs are mapped with write-combining. </p> <p>Summarizing the data transfer topic, we either have full coherency (where cpu cached memory mappings are the most efficient way to transfer data, with no explicit copying involved). Or we can copy data with the gpu (using snooped buffer objects), or with the cpu (either through memory mappings of the GTT or through the pwrite/pread ioctls). The kernel also supports explicit cpu cache flushing through the <code>set_domain</code> ioctl, which allows userspace to use cpu-cached memory maps even on platforms without coherency. But they're abysmally slow and hence not used in well-optimized code. </p> <h2>GPU Caches</h2> <p>Up to now we've only looked at cpu caches and presumed that the gpu is an opaque black box. But like already explained, gpus have tons of special-purpose caches and assorted tlbs, which need to be invalidated and flushed. First we will go trough the various gpu caches and look at how and when they're flushed. Then we'll look at how gpu cache domains are implemented in the kernel. </p> <p>Display caches are very simple: Data caches are always fifos, so never need to be flushed. And the assorted TLBs are in general invalidated on every vblank. The cpu mmio window into the GTT is also rather simple: No data caches need to be explicitly managed, only the tlb needs to be explicitly flush when updating PTEs by writing to a magic register afterwards. </p> <p>Much more complicated are the various special-purpose caches of the render core. Those all get flushed either explicitly (by setting the right bit in the relevant flush instruction) or implicitly as a side-effect of batch/ringbuffer commands. Important to note is that most of these caches are not fully coherent (safe for the new L3 gpu cache on Ivybridge), so writes do not invalidate cachelines in cpu caches, even when the respective gtt PTE has the snoop/caching bits set. Writes only invalidate the cpu caches once a cacheline gets evicted from the gpu caches. Hence the command streamer instruction which flush caches also are coherency barriers. </p> <p>One of the cornerstones of the original i915 GEM design was that the kernel explicitly keeps track of all the caches (render, texture, vertex, ...) an object is coherent in. It separately kept track of read and write domains, to optimize away flush for r/w caches if no cacheline should be dirty. But the code ended up being complicated and fragile, and thanks to tons of workarounds we need to flush most caches on many platforms way more often than necessary. Furthermore, userspace inserted it's own flushes to support e.g. rendering to textures, so the kernel wasn't even tracking properly which caches are clean. On top of that, the explicit tracking of dirty objects on the <code>flushing_list</code> to coalesce cache flushing operations added unnecessary delays, hurting workloads with tight gpu/cpu coupling. </p> <p>In the end this all ended up being a prime example of premature optimization. So for most parts the kernel now ignores the gpu domain values in relocations entries and instead unconditionally invalidates all caches before start a new batch and then flushes all write caches after the batch completes, but before the seqno is written and signaled with the CS interrupt. The big exception is workarounds, e.g. on Sandybridge writes from the command streamer used mostly for GL queries need global GTT entries shadowing the PPGTT PTEs. But all the in-kernel complexity to track dirty objects and gpu domains is mostly ripped out. The last remaining thing is to collapse all the different GEM domains into a simple boolean to track whether an object is cpu or gpu coherent. </p> <h2> Odds&amp;Sods </h2> <p>This now concludes our short tour of the i915.ko GEM implementation for submitting commands to the gpu. Two bigger topics are not covered though: Resetting the gpu when is stuck somewhere and how fairness is ensured by throttling command submission. Both are results of the cooperative nature of command submission on current intel gpus - batchbuffers cannot be preempted. Hence interactive responsiveness and fairness requires cooperation from userspace. And a stuch batchbuffer can only be stopped by resetting the entire gpu due to lack of preemption support. </p> <p>In both areas new ideas and improvements are floating around, so I've figured it best to cover them when improvements to the gpu reset code or the way we schedule batchbuffers land. </p>
  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">stuff by danvet</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>stuff by danvet</li>
          <li><a href="mailto:your-email@domain.com">your-email@domain.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/danvet"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">danvet</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/danvet"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">danvet</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
