<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>i915/GEM Crashcourse, Part 3</title>
  <meta name="description" content="In previous installments of this series we've looked at how the gpu can access memory and how to submit a workload to the gpu. Now we will look at some of th...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://yourdomain.com/2012/11/28/i915gem-crashcourse-part-3.html">
  <link rel="alternate" type="application/rss+xml" title="stuff by danvet" href="http://yourdomain.com/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">stuff by danvet</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">i915/GEM Crashcourse, Part 3</h1>
    <p class="post-meta"><time datetime="2012-11-28T23:36:00+01:00" itemprop="datePublished">Nov 28, 2012</time> â€¢ <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span itemprop="name">danvet</span></span></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>In previous installments of this series we've looked at <a href="http://blog.ffwll.ch/2012/10/i915gem-crashcourse.html">how the gpu can access memory</a> and <a href="http://blog.ffwll.ch/2012/11/neat-drmi915-stuff-for-38.html">how to submit a workload to the gpu</a>. Now we will look at some of the corner cases in more detail. See the <a href="http://blog.ffwll.ch/2013/01/i915gem-crashcourse-overview.html">i915/GEM crashcourse overview</a> for links to the other parts of this series.  </p> <a name='more'></a> <h2>GEM Memory Management Details</h2> <p>First we will look at the details of managing the gtt address space. For that the drm/i915 driver employs the <code>drm_mm</code> helper functions, which serves as a chunk allocator from a address space with a fixed size. It allows to constrain the alignment of an allocated block and also supports crazy gpu-specific guard-page constraints through an opaque per-allocation-block color attribute and a callback. Our driver uses that to ensure that so called snoopable and uncached regions on older generations are well-separated (by one page), since the hw prefetcher on some chip functions will fall over if it encounters the wrong type of memory while prefetching into a subsequent page. </p> <p>Now let's look at how GEM handles memory management corner cases. There are two important ways of running out of resources in GEM: There could be not enough space in the gtt for a given batch, this is the <code>ENOSPC</code> case in the code. Or we could run out of system memory (<code>ENOMEM</code>). One of the nice things of i915 GEM is that we use the kernel's shmemfs implementation to allocate backing storage for gem objects. The big benefit is that we get swap handling fro free. A bit a downside is that we don't have much control over how and when swapping happens, and also not really any control over how the backing storage pages get allocated. These downsides regularly result in some noises on irc channels about implementing a gemfs to fix these deficiencies, but with no patches merged thus far. </p> <h2>Caching GEM Buffer Objects</h2> <p>The first complication arises because GEM driver stack has a lot of buffer object caches. The most important object cache is the userspace cache in libdrm. Since setting up a gem object is expensive (allocating the shmemfs backing storage, but also cache flushing operations are required on some generations and setting up memory mappings on first use) it's advisable to keep them around for reuse. And submitting workloads to the gpu tends to require tons of temporary buffers to upload data and assorted gpu-specific state like shaders, so those buffers get recycled quickly.  </p> <p>Now if we run out of memory it would be good if the kernel can just drop the backing storage for these buffers on the floor instead of wasting time trying to swap their data out (or even failing to allocate memory, resulting in OOM-induced hilarity). Hence gem has a <code>gem_madvise</code> ioctl which allows a userspace cache to tell the kernel when a buffer is only kept around opportunistically with the <code>I915_MADV_DONTNEED</code>flag. The kernel is then allowed to reap the backing storage of such marked objects anytime it pleases. It can't though destroy the object tehmselves, since userspace still has a handle to them. Once userspace wants to reuse such a buffer, it mush tell the kernel by setting the <code>I915_MADV_WILLNEED</code> flag through the madvise ioctl. The kernel confirms that the object's backing storage is still around or whether it has been reaped. In the latter case userspace frees the object handle to also release the storage occupied by that in the kernel. If the userspace cache encounters such a reaped object it also walks all its cache buckets to free any other reaped objects. </p> <p>The kernel itself also tries to keep objects around on the gpu for as long as possible. Any object not used by the gpu is on the <code>inactive_list</code>. This is the list the kernel scans in LRU order when it needs to kick objects out of the gtt to free space for a different set of objects (e.g. for the ENOSPC case). In additions the kernel also keeps a list of objects which are not bound, but for which the backing storage is still pinned (and hence not allowed to be swapped out) on the <code>unbound_list</code>. This mostly serves to mitigate costly cache flushing operations, but the pinning of the backing storage is itself not free. We will talk later about why cache flushing is required in the coherency section in the next installment of this series. </p> <h2>Handling ENOSPC</h2> <p>Running out of gtt space requires us to select some victim buffer objects. Then wait for all access to the selected objects through the gtt to cease and then unbind them from the gpu address space to make room for the new objects. So let's take a closer look at how GEM handles buffer eviction from the gtt when it can't find a suitable free hole. Now setting up and tearing down pagetable entries isn't free, buffer objects vary a lot in size and for some access paths through the gtt we can only use the comparatively small mappable section of it. So just evicting objects from the inactive list in LRU order would be really inefficient, since we could end up unbinding lots of unrelated small objects sitting all over the place in the gtt, until we accidentally free a suitably large hole for a really big object. And if that big object needs to be in the contended mappable part, things are even worse. </p> <p>To avoid  trashing the gtt so badly GEM uses a eviction roaster, which is implemented with a few helper functions from the <code>drm_mm</code> allocator library: In a first step it scans through the inactive list and adds objects to the eviction roaster until a big enough free hole can be assembled. Then it walks the list of just scanned objects backwards to reconstruct the original allocator state and while doing that also marks any objects which fall into the selected hole as to-be-freed. In the last step it unbinds all the objects which are marked as to-be-freed, creating a suitable hole with the minimal amount of object rebinding. </p> <p>If no hole can be found by scanning the inactive list the eviction logic will also scan into the list of objects still being used by the gpu engines. Obviously it then needs to block for rendering to complete before it can unbind these objects. And if that doesn't work out, GEM returns <code>-ENOSPC</code> to userspace which indicates a bug in either GEM or the userspace driver part: Userspace must not construct batchbuffers which cannot fit into the GTT (presuming it's the only user of the gpu), and GEM must ensure that it can move anything else out of the way to execute such a giant batchbuffer. </p> <p>Now this works if we only need to put a single object into the gtt, e.g. when serving a pagefault for a gtt memory mapping. But when preparing a batchbuffer for command submission we need to make sure that all the buffer objects from an execbuf ioctl call are in the gtt. So we need to make sure that when reserving space for later objects, we don't kick out earlier objects which are also required to run this batchbuffer. To solve this, we need to mark objects as reserved and not evict reserved objects. In i915 GEM this is done by temporarily pinning objects, which every once in a while leads to interesting discussion because the semantics of such temporarily pinned buffers get mixed up with the semantics of buffers which are pinned for an indeterminate time, like scanout buffers. </p> <p>An funny exercise for the interested reader is to come up with optimal reservations schemes that allocate the gtt space for all buffers of an execbuf call at once (instead of buffer-by-buffer). This is mostly relevant on older generations which have sometimes rather massive alignment constraints on buffers. And create clever approximations of such optimal algorithms, which drm/i915 hopefully implements lready - otherwise patches highly welcome. Another good thing to ponder is the implications of a separate VRAM space (in combination with a GTT) which requires even more costly dma operations to move objects in, but in return has also much higher performance (i.e. the usual discrete gpu setup). One of the big upsides of a UMA (unified memory architecture) design as used on Intel integrated graphics is that there's no headache-inducing need to balance VRAM vs. GTT usage, which needs to take into account dma transfer costs and delays ... </p> <h2>Handling ENOMEM</h2> <p>Handling low memory situations is first and foremost done by the core vm, which tries to free up memory pages. In turn the core vm can call down int GEM (through the shrinker interface) to make more space available. </p> <p>Within the drm/i915 driver out of memory handling has a few tricks of its own, all of which are due to the rather simplistic locking scheme employed by drm/i915 GEM: We essentially only have one lock to protect all important GEM state, the <code>dev->struct_mutex</code>. Which means that we need to allocate tons of memory when holding that lock, in turn requiring that our memory shrinker callback only trylocks that all-encompassing mutex, since otherwise we might deadlock. We also need to tell the memory allocations functions through GFP flags that they shouldn't endlessly retry allocations and even launch the OOM killer, but instead fail the allocation if there's no memory and return immediately. Since most likely GEM itself is sitting on tons of memory we then try unpin a bunch of objects in such failure cases and retry the allocation ourselves. If that doesn't help, or if there's no buffer object that can be unpinned, we give up and return <code>-ENOMEM</code> to userspace. One side-effect of the trylock in the shrinker is that if any other thread than the allocator is hoggin our GEM mutex, we won't be able to unpin any gem objects, potentially causing a unnecessary OOM situation. </p> <p>Despite that this is rather hackish approach and not really how memory management is supposed to work, it seems to hold up rather well in reality. And recently (for kernel 3.7 and 3.8) Chris Wilson added some additional clever tricks to make it work for a bit longer when the kernel is tight on free memory. But in the long-term I expect that we need to revamp our locking scheme to be able to reliable free memory in low-memory situations. </p> <p>Another problem with the current low-memory handling is that the shrinker infrastructure is desinged to handle caches of equally-sized (filesystem) objects. It doesn't have any notion of objects of massively different sizes, which we try by reporting object counts in aggregate number of pages. And it also doesn't expect that when a shrinker runs, tons of memory suddenly shows up in the pagecache - which is what happens when we unpin the buffer object backing storage and give control over it back to shmemfs. </p> <p>In the next installement we will take a closer look at coherency issues and optimal ways to transfer data between the cpu and gpu. </p>
  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">stuff by danvet</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>stuff by danvet</li>
          <li><a href="mailto:your-email@domain.com">your-email@domain.com</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/danvet"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">danvet</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/danvet"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">danvet</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
